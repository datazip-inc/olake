version: "3"

services:
  amoro:
    image: apache/amoro
    container_name: amoro
    ports:
      - 8081:8081
      - 1630:1630
      - 1260:1260
    depends_on:
      postgres-init-amoro:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      minio:
        condition: service_started
    environment:
      - JVM_XMS=1024
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - SPARK_CONF_DIR=/opt/spark/conf
      - HADOOP_CONF_DIR=/etc/hadoop/conf
    networks:
      iceberg_net:
    volumes:
      # Persist Amoro warehouse/state across restarts
      - ./data/amoro-data:/tmp/warehouse
      # Provide the same JDBC Iceberg catalog defaults used by Spark in this local-test stack
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
      # Custom configuration to auto-load catalogs
      - ./amoro/config.yaml:/usr/local/amoro/conf/config.yaml:ro
      # Hadoop S3A config so Amoro/Iceberg reads s3a://warehouse/... from MinIO without 403
      - ./amoro/hadoop-conf:/etc/hadoop/conf:ro
    command: ["/entrypoint.sh", "ams"]
    tty: true
    stdin_open: true

  # Bootstrap a default local optimizer group + one optimizer on startup.
  # This makes the Amoro UI show the group immediately after `docker compose up`.
  amoro-init-optimizer:
    image: apache/amoro
    container_name: amoro-init-optimizer
    restart: unless-stopped
    networks:
      iceberg_net:
    depends_on:
      amoro:
        condition: service_started
    volumes:
      - ./amoro/init-optimizer.sh:/init-optimizer.sh:ro
    entrypoint: ["/bin/sh", "/init-optimizer.sh"]

  # Bootstrap the olake_iceberg catalog on startup (so it appears in the Catalogs UI automatically).
  amoro-init-catalog:
    image: apache/amoro
    container_name: amoro-init-catalog
    restart: "no"
    networks:
      iceberg_net:
    depends_on:
      amoro-init-optimizer:
        condition: service_started
    volumes:
      - ./amoro/init-catalog.sh:/init-catalog.sh:ro
    entrypoint: ["/bin/sh", "/init-catalog.sh"]

  lakekeeper:
    image: &lakekeeper-image ${LAKEKEEPER__SERVER_IMAGE:-quay.io/lakekeeper/catalog:latest-main}
    pull_policy: &lakekeeper-pull-policy always
    environment: &lakekeeper-environment
      - LAKEKEEPER__PG_ENCRYPTION_KEY=This-is-NOT-Secure!
      - LAKEKEEPER__PG_DATABASE_URL_READ=postgresql://iceberg:password@postgres:5432/iceberg
      - LAKEKEEPER__PG_DATABASE_URL_WRITE=postgresql://iceberg:password@postgres:5432/iceberg
      - LAKEKEEPER__AUTHZ_BACKEND=allowall
      # Externally taken from environment variables if set
      - LAKEKEEPER__OPENID_PROVIDER_URI
      - LAKEKEEPER__OPENID_AUDIENCE
      - LAKEKEEPER__OPENID_ADDITIONAL_ISSUERS
      - LAKEKEEPER__UI__OPENID_CLIENT_ID
      - LAKEKEEPER__UI__OPENID_SCOPE
    command: [ "serve" ]
    healthcheck:
      test: [ "CMD", "/home/nonroot/iceberg-catalog", "healthcheck" ]
      interval: 1s
      timeout: 10s
      retries: 3
      start_period: 3s
    depends_on:
      migrate:
        condition: service_completed_successfully
    ports:
      - "8181:8181"
    networks:
      iceberg_net:


  migrate:
    image: *lakekeeper-image
    pull_policy: *lakekeeper-pull-policy
    environment: *lakekeeper-environment
    restart: "no"
    command: [ "migrate" ]
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      iceberg_net:


  spark-iceberg:
    image: olakego/spark-connect:v0.0.1
    container_name: spark-iceberg
    networks:
      iceberg_net:
    depends_on:
      - minio
      - postgres
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    ports:
      - 8888:8888
      - 8088:8080
      - 10000:10000
      - 10001:10001
      - 15002:15002
    entrypoint: |
          /bin/sh -c "
          ./entrypoint.sh &&
          echo 'Starting Spark Connect server...' &&
          /opt/spark/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.5.2,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem &
          echo 'Starting Jupyter Notebook server...' &&
          export PYSPARK_DRIVER_PYTHON=jupyter-notebook &&
          export PYSPARK_DRIVER_PYTHON_OPTS=\"--notebook-dir=/home/iceberg/notebooks --ip='*' --NotebookApp.token='' --NotebookApp.password='' --port=8888 --no-browser --allow-root\" &&
          pyspark &
          while true; do sleep 30; done
          "

  minio:
    image: minio/minio:RELEASE.2025-04-03T14-56-28Z
    container_name: minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    networks:
      iceberg_net:
        aliases:
          - warehouse.minio
    ports:
      - 9001:9001
      - 9000:9000
    volumes:
      - ./data/minio-data:/data
    command: [ "server", "/data", "--console-address", ":9001" ]

  mc:
    depends_on:
      - minio
    image: minio/mc:RELEASE.2025-04-03T17-07-56Z
    container_name: mc
    networks:
      iceberg_net:
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    entrypoint: |
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      if ! /usr/bin/mc ls minio/warehouse > /dev/null 2>&1; then
        /usr/bin/mc mb minio/warehouse;
        /usr/bin/mc policy set public minio/warehouse;
      fi;
      tail -f /dev/null
      "

  postgres:
    image: postgres:15
    container_name: iceberg-postgres
    networks:
      iceberg_net:
    environment:
      - POSTGRES_USER=iceberg
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=iceberg
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "iceberg", "-d", "iceberg" ]
      interval: 2s
      timeout: 10s
      retries: 3
      start_period: 10s
    ports:
      - 5432:5432
    volumes:
      - ./data/postgres-data:/var/lib/postgresql/data
      - ./init-postgres.sql:/docker-entrypoint-initdb.d/init-postgres.sql:ro

  # Ensures the separate `amoro` database exists even when the Postgres data dir already exists
  # (docker-entrypoint-initdb.d only runs on first boot of an empty data dir).
  postgres-init-amoro:
    image: postgres:15
    container_name: postgres-init-amoro
    restart: "no"
    networks:
      iceberg_net:
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - PGPASSWORD=password
    volumes:
      - ./postgres-init-amoro.sh:/postgres-init-amoro.sh:ro
    entrypoint: ["/bin/sh", "/postgres-init-amoro.sh"]

  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    profiles:
      - hive
    networks:
      iceberg_net:
    ports:
      - "9083:9083"
    environment:
        - SERVICE_NAME=metastore
    depends_on:
      - postgres
      - spark-iceberg
    user: root
    volumes:
        - ./data/ivy-cache:/root/.ivy2
        - ./hive-site.conf:/opt/hive/conf/hive-site.xml
    entrypoint: |
      /bin/sh -c "
      # Wait for Spark to download JARs
      echo 'Waiting for Spark to download JARs...'
      while [ ! -f /root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar ] || \
            [ ! -f /root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar ] || \
            [ ! -f /root/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar ]; do
        echo 'Waiting for JARs to be available...'
        sleep 5
      done
      
      echo 'JARs found. Copying to Hive lib directory...'
      
      # Copy the JAR files to Hive's lib directory
      cp /root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar /opt/hive/lib/hadoop-aws-3.3.4.jar || { echo 'Failed to copy hadoop-aws-3.3.4.jar'; exit 1; }
      cp /root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar /opt/hive/lib/aws-java-sdk-bundle-1.12.262.jar || { echo 'Failed to copy aws-java-sdk-bundle-1.12.262.jar'; exit 1; }
      cp /root/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar /opt/hive/lib/postgresql-42.5.4.jar || { echo 'Failed to copy postgresql-42.5.4.jar'; exit 1; }
      
      # Verify files were copied successfully
      if [ ! -f /opt/hive/lib/hadoop-aws-3.3.4.jar ] || [ ! -f /opt/hive/lib/aws-java-sdk-bundle-1.12.262.jar ] || [ ! -f /opt/hive/lib/postgresql-42.5.4.jar ]; then
        echo 'JAR files not found in Hive lib directory. Exiting.'
        exit 1
      fi
      
      # Fix permissions for Hive directories
      chown -R hive:hive /opt/hive/lib
      
      echo 'JAR files copied to Hive lib directory. Starting hive-metastore...'
      
      # Start the hive metastore service
      sh -c "/entrypoint.sh"
      "

networks:
  iceberg_net:

volumes:
  postgres-data:
  minio-data: