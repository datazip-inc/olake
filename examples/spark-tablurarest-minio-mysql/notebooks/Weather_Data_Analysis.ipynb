{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Spark Session\n",
    "\n",
    "Create Spark session. Configuration is automatically loaded from `spark-defaults.conf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLake Weather Data Analysis with Apache Spark\n",
    "\n",
    "This notebook demonstrates querying Iceberg tables using Apache Spark after data has been synced from MySQL to Iceberg via OLake.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "All Spark and Iceberg configurations are loaded from mounted configuration files:\n",
    "- `/opt/spark/conf/spark-defaults.conf` - Spark and Iceberg settings\n",
    "- `/opt/spark/conf/core-site.xml` - Hadoop S3A configuration  \n",
    "- `/opt/spark/conf/catalog/iceberg.properties` - Iceberg REST catalog settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration files:\n",
      "  SPARK_CONF_DIR: /opt/spark/conf\n",
      "  HADOOP_CONF_DIR: /opt/spark/conf\n",
      "\n",
      "âœ… Spark session initialized successfully!\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Verify configuration files are mounted\n",
    "print(\"Configuration files:\")\n",
    "print(f\"  SPARK_CONF_DIR: {os.getenv('SPARK_CONF_DIR')}\")\n",
    "print(f\"  HADOOP_CONF_DIR: {os.getenv('HADOOP_CONF_DIR')}\")\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OLake Weather Analysis\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"\\nâœ… Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Configuration\n",
    "\n",
    "Check that Iceberg catalog configuration was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Iceberg Configuration:\n",
      "============================================================\n",
      "spark.hadoop.fs.s3a.connection.ssl.enabled: false\n",
      "spark.hadoop.fs.s3a.path.style.access: true\n",
      "spark.hadoop.fs.s3a.endpoint: http://minio:9090\n",
      "spark.hadoop.fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\n",
      "spark.sql.catalog.iceberg.warehouse: s3a://warehouse/\n",
      "spark.hadoop.fs.s3.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "spark.sql.catalog.iceberg.io-impl: org.apache.iceberg.hadoop.HadoopFileIO\n",
      "spark.hadoop.fs.s3a.access.key: minio\n",
      "spark.sql.catalog.iceberg.type: rest\n",
      "spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "spark.sql.catalog.iceberg: org.apache.iceberg.spark.SparkCatalog\n",
      "spark.sql.catalog.iceberg.uri: http://rest:8181\n",
      "spark.hadoop.fs.s3a.secret.key: ***MASKED***\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded Iceberg Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in spark.sparkContext.getConf().getAll():\n",
    "    if 'iceberg' in key.lower() or 's3' in key.lower():\n",
    "        # Mask sensitive values\n",
    "        if 'secret' in key.lower() or 'password' in key.lower():\n",
    "            value = '***MASKED***'\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Iceberg Catalog Connection\n",
    "\n",
    "Check available catalogs and namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Catalogs:\n",
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n",
      "\n",
      "Namespaces in Iceberg Catalog:\n",
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|          test_olake|\n",
      "|weather_sync_weather|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Available Catalogs:\")\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "print(\"\\nNamespaces in Iceberg Catalog:\")\n",
    "spark.sql(\"SHOW NAMESPACES IN iceberg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: List Tables in Weather Database\n",
    "\n",
    "After running the OLake sync job, you should see the `weather` table here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in weather namespace:\n",
      "+--------------------+---------+-----------+\n",
      "|           namespace|tableName|isTemporary|\n",
      "+--------------------+---------+-----------+\n",
      "|weather_sync_weather|  weather|      false|\n",
      "+--------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List tables in weather namespace\n",
    "print(\"Tables in weather namespace:\")\n",
    "spark.sql(\"SHOW TABLES IN iceberg.weather_sync_weather\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Query Weather Data\n",
    "\n",
    "Now let's query the actual weather data synced from MySQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 33,486\n",
      "\n",
      "First 10 rows:\n",
      "+---------------+----------+---------------+-------------+-----------------------+-------------+------------+--------------+------------+--------+----------------+-----+---------+----------+---------+----------+-------------------+------------+---------------+\n",
      "|temperature_avg|date_month|temperature_min|station_state|_olake_timestamp       |precipitation|date_week_of|wind_direction|station_city|_op_type|station_location|id   |date_year|date_full |_olake_id|wind_speed|_cdc_timestamp     |station_code|temperature_max|\n",
      "+---------------+----------+---------------+-------------+-----------------------+-------------+------------+--------------+------------+--------+----------------+-----+---------+----------+---------+----------+-------------------+------------+---------------+\n",
      "|39.0           |1         |32.0           |Alabama      |2025-11-19 20:35:46.381|0.0          |3           |33            |Birmingham  |c       |Birmingham, AL  |83716|2016     |2016-01-03|83716    |4.33      |2025-11-19 19:35:15|BHM         |46.0           |\n",
      "|39.0           |1         |31.0           |Alabama      |2025-11-19 20:35:46.381|0.0          |3           |32            |Huntsville  |c       |Huntsville, AL  |83717|2016     |2016-01-03|83717    |3.86      |2025-11-19 19:35:15|HSV         |47.0           |\n",
      "|46.0           |1         |41.0           |Alabama      |2025-11-19 20:35:46.381|0.16         |3           |35            |Mobile      |c       |Mobile, AL      |83718|2016     |2016-01-03|83718    |9.73      |2025-11-19 19:35:15|MOB         |51.0           |\n",
      "|45.0           |1         |38.0           |Alabama      |2025-11-19 20:35:46.381|0.0          |3           |32            |Montgomery  |c       |Montgomery, AL  |83719|2016     |2016-01-03|83719    |6.86      |2025-11-19 19:35:15|MGM         |52.0           |\n",
      "|34.0           |1         |29.0           |Alaska       |2025-11-19 20:35:46.381|0.01         |3           |19            |Anchorage   |c       |Anchorage, AK   |83720|2016     |2016-01-03|83720    |7.8       |2025-11-19 19:35:15|ANC         |38.0           |\n",
      "|38.0           |1         |31.0           |Alaska       |2025-11-19 20:35:46.381|0.09         |3           |9             |Annette     |c       |Annette, AK     |83721|2016     |2016-01-03|83721    |8.7       |2025-11-19 19:35:15|ANN         |44.0           |\n",
      "|30.0           |1         |24.0           |Alaska       |2025-11-19 20:35:46.381|0.05         |3           |9             |Bethel      |c       |Bethel, AK      |83722|2016     |2016-01-03|83722    |16.46     |2025-11-19 19:35:15|BET         |36.0           |\n",
      "|22.0           |1         |9.0            |Alaska       |2025-11-19 20:35:46.381|0.15         |3           |2             |Bettles     |c       |Bettles, AK     |83723|2016     |2016-01-03|83723    |3.1       |2025-11-19 19:35:15|BTT         |32.0           |\n",
      "|34.0           |1         |31.0           |Alaska       |2025-11-19 20:35:46.381|0.6          |3           |20            |Cold Bay    |c       |Cold Bay, AK    |83724|2016     |2016-01-03|83724    |9.1       |2025-11-19 19:35:15|CDB         |36.0           |\n",
      "|38.0           |1         |33.0           |Alaska       |2025-11-19 20:35:46.381|2.15         |3           |9             |Cordova     |c       |Cordova, AK     |83725|2016     |2016-01-03|83725    |9.76      |2025-11-19 19:35:15|CDV         |43.0           |\n",
      "+---------------+----------+---------------+-------------+-----------------------+-------------+------------+--------------+------------+--------+----------------+-----+---------+----------+---------+----------+-------------------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the weather table\n",
    "weather_df = spark.table(\"iceberg.weather_sync_weather.weather\")\n",
    "\n",
    "print(f\"Total records: {weather_df.count():,}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "weather_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather table schema:\n",
      "root\n",
      " |-- temperature_avg: float (nullable = true)\n",
      " |-- date_month: integer (nullable = true)\n",
      " |-- temperature_min: float (nullable = true)\n",
      " |-- station_state: string (nullable = true)\n",
      " |-- _olake_timestamp: timestamp (nullable = true)\n",
      " |-- precipitation: float (nullable = true)\n",
      " |-- date_week_of: integer (nullable = true)\n",
      " |-- wind_direction: integer (nullable = true)\n",
      " |-- station_city: string (nullable = true)\n",
      " |-- _op_type: string (nullable = true)\n",
      " |-- station_location: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date_year: integer (nullable = true)\n",
      " |-- date_full: string (nullable = true)\n",
      " |-- _olake_id: string (nullable = false)\n",
      " |-- wind_speed: float (nullable = true)\n",
      " |-- _cdc_timestamp: timestamp (nullable = true)\n",
      " |-- station_code: string (nullable = true)\n",
      " |-- temperature_max: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema\n",
    "print(\"Weather table schema:\")\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Analysis Examples\n",
    "\n",
    "Perform various analytics on the weather data using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Temperature by State (Top 10):\n",
      "+--------------+--------+------------+------------+------------+\n",
      "| station_state|avg_temp|avg_max_temp|avg_min_temp|record_count|\n",
      "+--------------+--------+------------+------------+------------+\n",
      "|   Puerto Rico|   81.36|       86.57|       75.62|         106|\n",
      "|        Hawaii|   76.38|       82.95|       69.37|         530|\n",
      "|       Florida|   73.93|       82.22|       65.17|        1272|\n",
      "|     Louisiana|   69.48|       78.93|       59.54|         522|\n",
      "|         Texas|   68.18|       78.85|       57.02|        2544|\n",
      "|       Alabama|   66.53|       77.23|       55.29|         424|\n",
      "|   Mississippi|   66.47|       77.34|       55.16|         742|\n",
      "|       Arizona|   66.29|       81.01|       51.06|         530|\n",
      "|       Georgia|   66.22|       77.44|        54.5|         636|\n",
      "|South Carolina|   65.19|       75.86|       54.05|         424|\n",
      "+--------------+--------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average temperature by state\n",
    "print(\"Average Temperature by State (Top 10):\")\n",
    "avg_temp_by_state = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        station_state,\n",
    "        ROUND(AVG(temperature_avg), 2) as avg_temp,\n",
    "        ROUND(AVG(temperature_max), 2) as avg_max_temp,\n",
    "        ROUND(AVG(temperature_min), 2) as avg_min_temp,\n",
    "        COUNT(*) as record_count\n",
    "    FROM iceberg.weather_sync_weather.weather\n",
    "    WHERE temperature_avg IS NOT NULL\n",
    "    GROUP BY station_state\n",
    "    ORDER BY avg_temp DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "avg_temp_by_state.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities with Highest Precipitation (Top 10):\n",
      "+--------------+-------------+-----------------+------------+\n",
      "|station_city  |station_state|avg_precipitation|measurements|\n",
      "+--------------+-------------+-----------------+------------+\n",
      "|Ketchikan     |Alaska       |2.6              |100         |\n",
      "|Quillayute    |Washington   |2.35             |102         |\n",
      "|Hilo          |Hawaii       |2.21             |106         |\n",
      "|Yakutat       |Alaska       |2.2              |100         |\n",
      "|Baton Rouge   |Louisiana    |1.98             |86          |\n",
      "|Mt. Washington|New Hampshire|1.93             |18          |\n",
      "|Astoria       |Oregon       |1.73             |98          |\n",
      "|Cordova       |Alaska       |1.71             |102         |\n",
      "|Annette       |Alaska       |1.69             |102         |\n",
      "|Redding       |California   |1.63             |60          |\n",
      "+--------------+-------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cities with highest precipitation\n",
    "print(\"Cities with Highest Precipitation (Top 10):\")\n",
    "high_precipitation = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        station_city,\n",
    "        station_state,\n",
    "        ROUND(AVG(precipitation), 2) as avg_precipitation,\n",
    "        COUNT(*) as measurements\n",
    "    FROM iceberg.weather_sync_weather.weather\n",
    "    WHERE precipitation IS NOT NULL AND precipitation > 0\n",
    "    GROUP BY station_city, station_state\n",
    "    ORDER BY avg_precipitation DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "high_precipitation.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Distribution Statistics:\n",
      "+-------------+------------+------------+----------------+------------+\n",
      "|total_records|coldest_temp|hottest_temp|overall_avg_temp|temp_std_dev|\n",
      "+-------------+------------+------------+----------------+------------+\n",
      "|        33486|       -35.0|       111.0|           56.09|        18.8|\n",
      "+-------------+------------+------------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temperature distribution statistics\n",
    "print(\"Temperature Distribution Statistics:\")\n",
    "temp_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        ROUND(MIN(temperature_min), 2) as coldest_temp,\n",
    "        ROUND(MAX(temperature_max), 2) as hottest_temp,\n",
    "        ROUND(AVG(temperature_avg), 2) as overall_avg_temp,\n",
    "        ROUND(STDDEV(temperature_avg), 2) as temp_std_dev\n",
    "    FROM iceberg.weather_sync_weather.weather\n",
    "    WHERE temperature_avg IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "temp_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Advanced Analytics with DataFrame API\n",
    "\n",
    "Use Spark's DataFrame API for more complex transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Temperature Trends:\n",
      "+---------+----------+--------+------------+\n",
      "|date_year|date_month|avg_temp|record_count|\n",
      "+---------+----------+--------+------------+\n",
      "|     2016|         1|   35.37|        3150|\n",
      "|     2016|         2|   40.87|        2520|\n",
      "|     2016|         3|   48.67|        2524|\n",
      "|     2016|         4|   53.89|        2530|\n",
      "|     2016|         5|   61.14|        3170|\n",
      "|     2016|         6|   71.56|        2524|\n",
      "|     2016|         7|   75.11|        3144|\n",
      "|     2016|         8|   74.41|        2536|\n",
      "|     2016|         9|   69.41|        2530|\n",
      "|     2016|        10|   59.56|        3170|\n",
      "|     2016|        11|   49.76|        2518|\n",
      "|     2016|        12|   35.65|        2536|\n",
      "|     2017|         1|   39.77|         634|\n",
      "+---------+----------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, count, round as spark_round\n",
    "\n",
    "# Monthly temperature trends by year\n",
    "monthly_trends = weather_df \\\n",
    "    .filter(col(\"temperature_avg\").isNotNull()) \\\n",
    "    .groupBy(\"date_year\", \"date_month\") \\\n",
    "    .agg(\n",
    "        spark_round(avg(\"temperature_avg\"), 2).alias(\"avg_temp\"),\n",
    "        count(\"*\").alias(\"record_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"date_year\", \"date_month\")\n",
    "\n",
    "print(\"Monthly Temperature Trends:\")\n",
    "monthly_trends.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind Speed Analysis by State (Top 10):\n",
      "+-------------+--------------+--------------+--------------+\n",
      "|station_state|avg_wind_speed|max_wind_speed|min_wind_speed|\n",
      "+-------------+--------------+--------------+--------------+\n",
      "|New Hampshire|         18.02|          61.1|          1.58|\n",
      "|       Hawaii|          8.97|         19.22|           1.3|\n",
      "|Massachusetts|          8.75|         15.84|           0.0|\n",
      "| South Dakota|          8.47|         16.87|          2.82|\n",
      "| North Dakota|          8.46|          17.5|           1.4|\n",
      "|     Oklahoma|          8.46|         16.08|           1.8|\n",
      "|       Kansas|          8.25|          16.2|          2.05|\n",
      "|     Nebraska|          8.05|         15.64|          2.62|\n",
      "|         Iowa|          8.01|         14.33|           2.4|\n",
      "|        Texas|          7.73|          20.3|          0.55|\n",
      "+-------------+--------------+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wind speed analysis by state\n",
    "from pyspark.sql.functions import max as spark_max, min as spark_min\n",
    "\n",
    "wind_analysis = weather_df \\\n",
    "    .filter(col(\"wind_speed\").isNotNull()) \\\n",
    "    .groupBy(\"station_state\") \\\n",
    "    .agg(\n",
    "        spark_round(avg(\"wind_speed\"), 2).alias(\"avg_wind_speed\"),\n",
    "        spark_round(spark_max(\"wind_speed\"), 2).alias(\"max_wind_speed\"),\n",
    "        spark_round(spark_min(\"wind_speed\"), 2).alias(\"min_wind_speed\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"avg_wind_speed\").desc())\n",
    "\n",
    "print(\"Wind Speed Analysis by State (Top 10):\")\n",
    "wind_analysis.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Data Export (Optional)\n",
    "\n",
    "Convert query results to pandas for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas DataFrame:\n",
      "    station_state  avg_temp  avg_max_temp  avg_min_temp  record_count\n",
      "0     Puerto Rico     81.36         86.57         75.62           106\n",
      "1          Hawaii     76.38         82.95         69.37           530\n",
      "2         Florida     73.93         82.22         65.17          1272\n",
      "3       Louisiana     69.48         78.93         59.54           522\n",
      "4           Texas     68.18         78.85         57.02          2544\n",
      "5         Alabama     66.53         77.23         55.29           424\n",
      "6     Mississippi     66.47         77.34         55.16           742\n",
      "7         Arizona     66.29         81.01         51.06           530\n",
      "8         Georgia     66.22         77.44         54.50           636\n",
      "9  South Carolina     65.19         75.86         54.05           424\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas for visualization (only for small datasets)\n",
    "avg_temp_pandas = avg_temp_by_state.toPandas()\n",
    "print(\"Converted to Pandas DataFrame:\")\n",
    "print(avg_temp_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. âœ… Loading Spark configuration from external files \n",
    "2. âœ… Connecting Spark to Iceberg REST catalog\n",
    "3. âœ… Querying weather data synced from MySQL via OLake\n",
    "4. âœ… Performing aggregations and analytics using Spark SQL\n",
    "5. âœ… Using DataFrame API for complex transformations\n",
    "\n",
    "### Configuration Architecture\n",
    "\n",
    "This setup follows:\n",
    "- Spark: Uses `/opt/spark/conf` for configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Stop the Spark session when completely done\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§ª Testing Iceberg Catalog...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available catalogs:\")\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
