{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Spark Session\n",
    "\n",
    "Create Spark session. Configuration is automatically loaded from `spark-defaults.conf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLake Weather Data Analysis with Apache Spark\n",
    "\n",
    "This notebook demonstrates querying Iceberg tables using Apache Spark after data has been synced from MySQL to Iceberg via OLake.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "All Spark and Iceberg configurations are loaded from mounted configuration files:\n",
    "- `/opt/spark/conf/spark-defaults.conf` - Spark and Iceberg settings\n",
    "- `/opt/spark/conf/core-site.xml` - Hadoop S3A configuration  \n",
    "- `/opt/spark/conf/catalog/iceberg.properties` - Iceberg REST catalog settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Verify configuration files are mounted\n",
    "print(\"Configuration files:\")\n",
    "print(f\"  SPARK_CONF_DIR: {os.getenv('SPARK_CONF_DIR')}\")\n",
    "print(f\"  HADOOP_CONF_DIR: {os.getenv('HADOOP_CONF_DIR')}\")\n",
    "\n",
    "# Create Spark session - configs loaded from spark-defaults.conf\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OLake Weather Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"\\n✅ Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Configuration\n",
    "\n",
    "Check that Iceberg catalog configuration was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loaded Spark configurations related to Iceberg\n",
    "print(\"Loaded Iceberg Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in spark.sparkContext.getConf().getAll():\n",
    "    if 'iceberg' in key.lower() or 's3' in key.lower():\n",
    "        # Mask sensitive values\n",
    "        if 'secret' in key.lower() or 'password' in key.lower():\n",
    "            value = '***MASKED***'\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Iceberg Catalog Connection\n",
    "\n",
    "Check available catalogs and namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available catalogs\n",
    "print(\"Available Catalogs:\")\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "# Show namespaces in Iceberg catalog\n",
    "print(\"\\nNamespaces in Iceberg Catalog:\")\n",
    "spark.sql(\"SHOW NAMESPACES IN iceberg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: List Tables in Weather Database\n",
    "\n",
    "After running the OLake sync job, you should see the `weather` table here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in weather namespace\n",
    "print(\"Tables in weather namespace:\")\n",
    "spark.sql(\"SHOW TABLES IN iceberg.weather\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Query Weather Data\n",
    "\n",
    "Now let's query the actual weather data synced from MySQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the weather table\n",
    "weather_df = spark.table(\"iceberg.weather.weather\")\n",
    "\n",
    "print(f\"Total records: {weather_df.count():,}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "weather_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "print(\"Weather table schema:\")\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Analysis Examples\n",
    "\n",
    "Perform various analytics on the weather data using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average temperature by state\n",
    "print(\"Average Temperature by State (Top 10):\")\n",
    "avg_temp_by_state = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        station_state,\n",
    "        ROUND(AVG(temperature_avg), 2) as avg_temp,\n",
    "        ROUND(AVG(temperature_max), 2) as avg_max_temp,\n",
    "        ROUND(AVG(temperature_min), 2) as avg_min_temp,\n",
    "        COUNT(*) as record_count\n",
    "    FROM iceberg.weather.weather\n",
    "    WHERE temperature_avg IS NOT NULL\n",
    "    GROUP BY station_state\n",
    "    ORDER BY avg_temp DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "avg_temp_by_state.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cities with highest precipitation\n",
    "print(\"Cities with Highest Precipitation (Top 10):\")\n",
    "high_precipitation = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        station_city,\n",
    "        station_state,\n",
    "        ROUND(AVG(precipitation), 2) as avg_precipitation,\n",
    "        COUNT(*) as measurements\n",
    "    FROM iceberg.weather.weather\n",
    "    WHERE precipitation IS NOT NULL AND precipitation > 0\n",
    "    GROUP BY station_city, station_state\n",
    "    ORDER BY avg_precipitation DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "high_precipitation.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature distribution statistics\n",
    "print(\"Temperature Distribution Statistics:\")\n",
    "temp_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        ROUND(MIN(temperature_min), 2) as coldest_temp,\n",
    "        ROUND(MAX(temperature_max), 2) as hottest_temp,\n",
    "        ROUND(AVG(temperature_avg), 2) as overall_avg_temp,\n",
    "        ROUND(STDDEV(temperature_avg), 2) as temp_std_dev\n",
    "    FROM iceberg.weather.weather\n",
    "    WHERE temperature_avg IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "temp_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Advanced Analytics with DataFrame API\n",
    "\n",
    "Use Spark's DataFrame API for more complex transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count, round as spark_round\n",
    "\n",
    "# Monthly temperature trends by year\n",
    "monthly_trends = weather_df \\\n",
    "    .filter(col(\"temperature_avg\").isNotNull()) \\\n",
    "    .groupBy(\"date_year\", \"date_month\") \\\n",
    "    .agg(\n",
    "        spark_round(avg(\"temperature_avg\"), 2).alias(\"avg_temp\"),\n",
    "        count(\"*\").alias(\"record_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"date_year\", \"date_month\")\n",
    "\n",
    "print(\"Monthly Temperature Trends:\")\n",
    "monthly_trends.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind speed analysis by state\n",
    "from pyspark.sql.functions import max as spark_max, min as spark_min\n",
    "\n",
    "wind_analysis = weather_df \\\n",
    "    .filter(col(\"wind_speed\").isNotNull()) \\\n",
    "    .groupBy(\"station_state\") \\\n",
    "    .agg(\n",
    "        spark_round(avg(\"wind_speed\"), 2).alias(\"avg_wind_speed\"),\n",
    "        spark_round(spark_max(\"wind_speed\"), 2).alias(\"max_wind_speed\"),\n",
    "        spark_round(spark_min(\"wind_speed\"), 2).alias(\"min_wind_speed\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"avg_wind_speed\").desc())\n",
    "\n",
    "print(\"Wind Speed Analysis by State (Top 10):\")\n",
    "wind_analysis.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Data Export (Optional)\n",
    "\n",
    "Convert query results to pandas for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for visualization (only for small datasets)\n",
    "avg_temp_pandas = avg_temp_by_state.toPandas()\n",
    "print(\"Converted to Pandas DataFrame:\")\n",
    "print(avg_temp_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Loading Spark configuration from external files \n",
    "2. ✅ Connecting Spark to Iceberg REST catalog\n",
    "3. ✅ Querying weather data synced from MySQL via OLake\n",
    "4. ✅ Performing aggregations and analytics using Spark SQL\n",
    "5. ✅ Using DataFrame API for complex transformations\n",
    "\n",
    "### Configuration Architecture\n",
    "\n",
    "This setup follows:\n",
    "- Spark: Uses `/opt/spark/conf` for configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Stop the Spark session when completely done\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
